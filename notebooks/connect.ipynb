{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared Americas - UC 0112-222456-dx1cy12y\n",
      "Shared Autoscaling Americas 0807-225846-motto493\n",
      "Shared Compute - UC - 14.3 0118-165440-l8erfx3g\n",
      "Ben Doan's Cluster 0412-130448-a3cje954\n",
      "Brian McConnell's Personal Compute Cluster 0729-081447-8slgfxzk\n",
      "chris_koester 0729-212854-o3mbohsl\n",
      "dlt-execution-b315eace-7c41-4ca6-a0c5-e3412475b9c4 0731-213755-ibnnqpzu\n",
      "Duncan Davis's Cluster 0730-160154-fr9o1gx2\n",
      "josh.melton's Cluster 0716-142217-g76pr80o\n",
      "Karthiga Mahalingam's Cluster 0724-145234-jw8zvy2q\n",
      "msh-cpu-mn-1 0717-134700-77r6inm7\n",
      "Product Matching Demo Cluster 0718-131735-koqsgvc6\n",
      "qyu_multi-nodes 0715-002328-cigi8x5x\n",
      "Scott McKean's DBAcademy Cluster 0731-145937-428fkr7h\n",
      "Sindhu Murugavel's Cluster 0731-205056-b583zw4d\n",
      "Thiago da Hora's Cluster 0729-192451-4qrwhqs7\n",
      "ys_demo 0730-221634-ajmzd49c\n"
     ]
    }
   ],
   "source": [
    "# Use the databricks SDK to list out the running clusters\n",
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "for c in w.clusters.list():\n",
    "  if \"RUNNING\" in str(c.state):\n",
    "    print(c.cluster_name, c.cluster_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SparkConnectGrpcException",
     "evalue": "(org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle 25621fa6-ee24-4947-8d62-823708f0f12f is invalid. Session was closed. SQLSTATE: HY000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkConnectGrpcException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# We can now use spark connect\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples.nyctaxi.trips\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1159\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:903\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    888\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    889\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m             },\n\u001b[1;32m    893\u001b[0m         )\n\u001b[1;32m    895\u001b[0m table, _ \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mShowString\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_truncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 903\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mas_py()\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1866\u001b[0m, in \u001b[0;36mDataFrame._to_table\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_table\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpa.Table\u001b[39m\u001b[38;5;124m\"\u001b[39m, Optional[StructType]]:\n\u001b[1;32m   1865\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mto_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m-> 1866\u001b[0m     table, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (table, schema)\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:992\u001b[0m, in \u001b[0;36mSparkConnectClient.to_table\u001b[0;34m(self, plan, observations)\u001b[0m\n\u001b[1;32m    990\u001b[0m req \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_plan_request_with_metadata()\n\u001b[1;32m    991\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mCopyFrom(plan)\n\u001b[0;32m--> 992\u001b[0m table, schema, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table, schema\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1624\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[0m\n\u001b[1;32m   1621\u001b[0m schema: Optional[StructType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1622\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1624\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1601\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, extra_request_metadata)\u001b[0m\n\u001b[1;32m   1599\u001b[0m                     \u001b[38;5;28;01myield from\u001b[39;00m handle_response(b)\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1601\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1910\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 1910\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1912\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1985\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   1982\u001b[0m             info \u001b[38;5;241m=\u001b[39m error_details_pb2\u001b[38;5;241m.\u001b[39mErrorInfo()\n\u001b[1;32m   1983\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n\u001b[0;32m-> 1985\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[1;32m   1986\u001b[0m                 info,\n\u001b[1;32m   1987\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   1988\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n\u001b[1;32m   1989\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n\u001b[1;32m   1990\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mSparkConnectGrpcException\u001b[0m: (org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle 25621fa6-ee24-4947-8d62-823708f0f12f is invalid. Session was closed. SQLSTATE: HY000"
     ]
    }
   ],
   "source": [
    "# Because we have multiple cluster ids, we need to set the cluster_id attribute\n",
    "from databricks.sdk.core import Config\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "# I am using the config to setup a session\n",
    "config = Config()\n",
    "config.cluster_id = '0731-145937-428fkr7h'\n",
    "spark = DatabricksSession.builder.sdkConfig(config).getOrCreate()\n",
    "\n",
    "# We can now use spark connect\n",
    "df = spark.read.table(\"samples.nyctaxi.trips\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "keyword = 'lithium brine'\n",
    "base_url = \"http://export.arxiv.org/api/query\"\n",
    "max_results = 10\n",
    "query = f\"search_query=all:{keyword.replace(' ','+')}&start=0&max_results={max_results}\"\n",
    "url = f\"{base_url}?{query}\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_data = response.content\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the XML data\n",
    "root = ET.fromstring(xml_data)\n",
    "\n",
    "data = []\n",
    "for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "    title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "    url = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
    "    data.append({\"title\": title, \"url\": url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(data).createOrReplaceTempView(\"spark_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SparkConnectGrpcException",
     "evalue": "(org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle 25621fa6-ee24-4947-8d62-823708f0f12f is invalid. Session was closed. SQLSTATE: HY000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkConnectGrpcException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/session.py:594\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    592\u001b[0m         _schema \u001b[38;5;241m=\u001b[39m StructType()\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, _schema)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 594\u001b[0m     _schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _cols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cast(\u001b[38;5;28mint\u001b[39m, _num_cols) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(_cols):\n\u001b[1;32m    597\u001b[0m         _num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(_cols)\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/session.py:388\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    380\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    381\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    384\u001b[0m (\n\u001b[1;32m    385\u001b[0m     infer_dict_as_struct,\n\u001b[1;32m    386\u001b[0m     infer_array_from_first_element,\n\u001b[1;32m    387\u001b[0m     prefer_timestamp_ntz,\n\u001b[0;32m--> 388\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.pyspark.inferNestedDictAsStruct.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.timestampType\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(\n\u001b[1;32m    394\u001b[0m     _merge_type,\n\u001b[1;32m    395\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m     ),\n\u001b[1;32m    405\u001b[0m )\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1686\u001b[0m, in \u001b[0;36mSparkConnectClient.get_configs\u001b[0;34m(self, *keys)\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_configs\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mkeys: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m   1685\u001b[0m     op \u001b[38;5;241m=\u001b[39m pb2\u001b[38;5;241m.\u001b[39mConfigRequest\u001b[38;5;241m.\u001b[39mOperation(get\u001b[38;5;241m=\u001b[39mpb2\u001b[38;5;241m.\u001b[39mConfigRequest\u001b[38;5;241m.\u001b[39mGet(keys\u001b[38;5;241m=\u001b[39mkeys))\n\u001b[0;32m-> 1686\u001b[0m     configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpairs)\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(configs\u001b[38;5;241m.\u001b[39mget(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys)\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1723\u001b[0m, in \u001b[0;36mSparkConnectClient.config\u001b[0;34m(self, operation)\u001b[0m\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid state during retry exception handling.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1910\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 1910\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1912\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "File \u001b[0;32m~/Repos/databricks-sdk-demo/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1985\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   1982\u001b[0m             info \u001b[38;5;241m=\u001b[39m error_details_pb2\u001b[38;5;241m.\u001b[39mErrorInfo()\n\u001b[1;32m   1983\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n\u001b[0;32m-> 1985\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[1;32m   1986\u001b[0m                 info,\n\u001b[1;32m   1987\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   1988\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n\u001b[1;32m   1989\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n\u001b[1;32m   1990\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mSparkConnectGrpcException\u001b[0m: (org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle 25621fa6-ee24-4947-8d62-823708f0f12f is invalid. Session was closed. SQLSTATE: HY000"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(url='http://arxiv.org/abs/2402.07000v2')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"url\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
